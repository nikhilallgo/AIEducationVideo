{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Complete AI Services with SAM Segmentation - OCR, Segmentation, and Text-to-Speech\n",
        "# This is a complete solution using Segment Anything Model instead of YOLO\n",
        "# Run this in Google Colab for instant public access\n",
        "\n",
        "# Install required packages\n",
        "!pip install gradio pytesseract pillow gtts pyttsx3 edge-tts opencv-python-headless numpy requests asyncio\n",
        "!pip install segment-anything torch torchvision\n",
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "# Additional system dependencies for Tesseract\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-eng espeak espeak-data libespeak1 libespeak-dev\n",
        "\n",
        "# Download SAM models\n",
        "print(\"ğŸ“¥ Downloading SAM model checkpoints...\")\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "print(\"âœ… SAM models downloaded successfully!\")\n",
        "\n",
        "import gradio as gr\n",
        "import pytesseract\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import numpy as np\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "import asyncio\n",
        "import subprocess\n",
        "import sys\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "# Global variables\n",
        "sam_models = {}  # Store multiple SAM models\n",
        "current_sam_model = \"sam_vit_b\"\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "models_loaded = {\n",
        "    \"sam_vit_b\": False,\n",
        "    \"sam_vit_l\": False,\n",
        "    \"sam_vit_h\": False\n",
        "}\n",
        "tts_engines = {\"gtts\": True, \"pyttsx3\": False, \"edge-tts\": False}  # Track TTS engine availability\n",
        "\n",
        "def initialize_tts_engines():\n",
        "    \"\"\"Initialize and check available TTS engines\"\"\"\n",
        "    global tts_engines\n",
        "\n",
        "    # Google TTS (always available with internet)\n",
        "    tts_engines[\"gtts\"] = True\n",
        "\n",
        "    # Try to initialize pyttsx3 (offline TTS)\n",
        "    try:\n",
        "        import pyttsx3\n",
        "        engine = pyttsx3.init()\n",
        "        engine.stop()  # Stop any existing engine\n",
        "        tts_engines[\"pyttsx3\"] = True\n",
        "        print(\"âœ… Pyttsx3 TTS engine available\")\n",
        "    except Exception as e:\n",
        "        tts_engines[\"pyttsx3\"] = False\n",
        "        print(f\"âš ï¸ Pyttsx3 TTS not available: {e}\")\n",
        "\n",
        "    # Try to check edge-tts availability\n",
        "    try:\n",
        "        import edge_tts\n",
        "        tts_engines[\"edge-tts\"] = True\n",
        "        print(\"âœ… Edge TTS engine available\")\n",
        "    except Exception as e:\n",
        "        tts_engines[\"edge-tts\"] = False\n",
        "        print(f\"âš ï¸ Edge TTS not available: {e}\")\n",
        "\n",
        "    return f\"TTS Engines Status:\\nâœ… Google TTS: Available\\n{'âœ…' if tts_engines['pyttsx3'] else 'âŒ'} Pyttsx3: {'Available' if tts_engines['pyttsx3'] else 'Not Available'}\\n{'âœ…' if tts_engines['edge-tts'] else 'âŒ'} Edge TTS: {'Available' if tts_engines['edge-tts'] else 'Not Available'}\"\n",
        "\n",
        "def initialize_sam_models(model_name=\"sam_vit_b\"):\n",
        "    \"\"\"Initialize specific SAM model\"\"\"\n",
        "    global sam_models, models_loaded\n",
        "\n",
        "    model_map = {\n",
        "        \"sam_vit_b\": {\n",
        "            \"checkpoint\": \"sam_vit_b_01ec64.pth\",\n",
        "            \"model_type\": \"vit_b\",\n",
        "            \"description\": \"Base model - Fast and efficient\"\n",
        "        },\n",
        "        \"sam_vit_l\": {\n",
        "            \"checkpoint\": \"sam_vit_l_0b3195.pth\",\n",
        "            \"model_type\": \"vit_l\",\n",
        "            \"description\": \"Large model - Better accuracy\"\n",
        "        },\n",
        "        \"sam_vit_h\": {\n",
        "            \"checkpoint\": \"sam_vit_h_4b8939.pth\",\n",
        "            \"model_type\": \"vit_h\",\n",
        "            \"description\": \"Huge model - Best accuracy\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if model_name not in model_map:\n",
        "        return f\"âŒ Unknown SAM model: {model_name}\"\n",
        "\n",
        "    try:\n",
        "        print(f\"ğŸ”„ Loading {model_name} model...\")\n",
        "\n",
        "        model_info = model_map[model_name]\n",
        "        checkpoint_path = model_info[\"checkpoint\"]\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            return f\"âŒ Model checkpoint not found: {checkpoint_path}. Please download it first.\"\n",
        "\n",
        "        # Load SAM model\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        sam = sam_model_registry[model_info[\"model_type\"]](checkpoint=checkpoint_path)\n",
        "        sam.to(device=device)\n",
        "\n",
        "        sam_models[model_name] = {\n",
        "            \"model\": sam,\n",
        "            \"predictor\": SamPredictor(sam),\n",
        "            \"mask_generator\": SamAutomaticMaskGenerator(sam),\n",
        "            \"device\": device\n",
        "        }\n",
        "        models_loaded[model_name] = True\n",
        "\n",
        "        result = f\"âœ… {model_name.upper()} loaded successfully!\\n\"\n",
        "        result += f\"ğŸ“Š Model: Segment Anything {model_info['model_type'].upper()}\\n\"\n",
        "        result += f\"ğŸ¯ Description: {model_info['description']}\\n\"\n",
        "        result += f\"ğŸ’¾ Device: {device.upper()}\\n\"\n",
        "        result += f\"ğŸ”§ Status: Ready for segmentation\"\n",
        "\n",
        "        print(f\"âœ… {model_name} model loaded successfully on {device}!\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        models_loaded[model_name] = False\n",
        "        error_msg = f\"âŒ Error loading {model_name}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "def load_all_sam_models():\n",
        "    \"\"\"Load all available SAM models\"\"\"\n",
        "    results = \"ğŸš€ Loading all SAM models...\\n\\n\"\n",
        "\n",
        "    for model_name in [\"sam_vit_b\"]:  # Start with base model only for speed\n",
        "        result = initialize_sam_models(model_name)\n",
        "        results += f\"**{model_name.upper()}:**\\n{result}\\n\\n\"\n",
        "\n",
        "    return results\n",
        "\n",
        "def perform_ocr(image, language=\"eng\"):\n",
        "    \"\"\"Extract text from image using Tesseract OCR\"\"\"\n",
        "    if image is None:\n",
        "        return \"âŒ No image provided\", \"\", 0, 0\n",
        "\n",
        "    try:\n",
        "        # Perform OCR\n",
        "        extracted_text = pytesseract.image_to_string(image, lang=language)\n",
        "\n",
        "        # Get additional OCR data for confidence\n",
        "        ocr_data = pytesseract.image_to_data(image, lang=language, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidences = [int(conf) for conf in ocr_data['conf'] if int(conf) > 0]\n",
        "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "\n",
        "        # Count words\n",
        "        word_count = len(extracted_text.split()) if extracted_text.strip() else 0\n",
        "\n",
        "        if extracted_text.strip():\n",
        "            status = f\"âœ… OCR completed successfully! Found {word_count} words with {avg_confidence:.1f}% confidence\"\n",
        "        else:\n",
        "            status = \"âš ï¸ No text found in the image\"\n",
        "\n",
        "        return status, extracted_text.strip(), word_count, round(avg_confidence, 1)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ OCR Error: {str(e)}\", \"\", 0, 0\n",
        "\n",
        "def segment_objects_with_sam(image, model_name=\"sam_vit_b\", min_area=500, max_segments=50):\n",
        "    \"\"\"Segment objects using SAM and return masks and visualizations\"\"\"\n",
        "    if image is None:\n",
        "        return \"âŒ No image provided\", \"\", image, image, 0, 0\n",
        "\n",
        "    if model_name not in sam_models or not models_loaded[model_name]:\n",
        "        return f\"âŒ SAM model {model_name} not loaded. Please load the model first.\", \"\", image, image, 0, 0\n",
        "\n",
        "    try:\n",
        "        # Get the selected model\n",
        "        sam_info = sam_models[model_name]\n",
        "        mask_generator = sam_info[\"mask_generator\"]\n",
        "\n",
        "        # Convert PIL to numpy array\n",
        "        image_array = np.array(image)\n",
        "\n",
        "        # Generate masks automatically\n",
        "        print(f\"ğŸ”„ Generating masks with {model_name}...\")\n",
        "        masks = mask_generator.generate(image_array)\n",
        "\n",
        "        # Filter masks by area\n",
        "        filtered_masks = [mask for mask in masks if mask['area'] >= min_area]\n",
        "\n",
        "        # Sort by area (largest first) and limit number\n",
        "        filtered_masks = sorted(filtered_masks, key=lambda x: x['area'], reverse=True)[:max_segments]\n",
        "\n",
        "        # Create visualizations\n",
        "        segmented_image, overlay_image = create_sam_visualizations(image_array, filtered_masks)\n",
        "\n",
        "        # Create text summary\n",
        "        if filtered_masks:\n",
        "            result_text = f\"ğŸ¤– **{model_name.upper()} Segmentation Results**\\n\\n\"\n",
        "            result_text += f\"ğŸ¯ **Found {len(filtered_masks)} segments**\\n\\n\"\n",
        "\n",
        "            # Add summary statistics\n",
        "            total_area = sum(mask['area'] for mask in filtered_masks)\n",
        "            image_area = image_array.shape[0] * image_array.shape[1]\n",
        "            coverage = (total_area / image_area) * 100\n",
        "\n",
        "            result_text += \"ğŸ“Š **Segmentation Statistics:**\\n\"\n",
        "            result_text += f\"â€¢ **Total Segments**: {len(filtered_masks)}\\n\"\n",
        "            result_text += f\"â€¢ **Image Coverage**: {coverage:.1f}%\\n\"\n",
        "            result_text += f\"â€¢ **Largest Segment**: {filtered_masks[0]['area']:,} pixels\\n\"\n",
        "            result_text += f\"â€¢ **Smallest Segment**: {filtered_masks[-1]['area']:,} pixels\\n\\n\"\n",
        "\n",
        "            # Add detailed list\n",
        "            result_text += f\"ğŸ“‹ **Detailed Segment List:**\\n\"\n",
        "            for i, mask in enumerate(filtered_masks[:10], 1):  # Show top 10\n",
        "                area = mask['area']\n",
        "                stability_score = mask.get('stability_score', 0)\n",
        "                bbox = mask['bbox']  # [x, y, w, h]\n",
        "\n",
        "                result_text += f\"{i}. **Segment {i}** ğŸ“ Area: {area:,}px | \"\n",
        "                result_text += f\"ğŸ“Š Stability: {stability_score:.3f} | \"\n",
        "                result_text += f\"ğŸ“¦ Box: ({int(bbox[0])}, {int(bbox[1])}) {int(bbox[2])}Ã—{int(bbox[3])}\\n\"\n",
        "\n",
        "            if len(filtered_masks) > 10:\n",
        "                result_text += f\"... and {len(filtered_masks) - 10} more segments\\n\"\n",
        "\n",
        "            status = f\"âœ… {model_name.upper()} segmentation completed! Found {len(filtered_masks)} segments\"\n",
        "        else:\n",
        "            result_text = f\"ğŸ¤– **{model_name.upper()} Segmentation Results**\\n\\n\"\n",
        "            result_text += \"ğŸ” No segments found with the current area threshold.\\n\\n\"\n",
        "            result_text += \"ğŸ’¡ **Try:**\\n\"\n",
        "            result_text += \"- Lowering the minimum area threshold\\n\"\n",
        "            result_text += \"- Using a different SAM model\\n\"\n",
        "            result_text += \"- Using an image with more distinct objects\"\n",
        "\n",
        "            status = f\"âš ï¸ {model_name.upper()}: No segments found with current threshold\"\n",
        "            segmented_image = image\n",
        "            overlay_image = image\n",
        "\n",
        "        return status, result_text, segmented_image, overlay_image, len(filtered_masks), len(filtered_masks)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ {model_name.upper()} Segmentation Error: {str(e)}\"\n",
        "        return error_msg, \"\", image, image, 0, 0\n",
        "\n",
        "def create_sam_visualizations(image_array, masks):\n",
        "    \"\"\"Create visualizations for SAM segmentation results\"\"\"\n",
        "\n",
        "    # Create segmented image (each segment with different color)\n",
        "    segmented_image = image_array.copy()\n",
        "    overlay_image = image_array.copy()\n",
        "\n",
        "    # Color palette for segments\n",
        "    colors = [\n",
        "        [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255],\n",
        "        [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128], [128, 128, 0],\n",
        "        [128, 0, 128], [0, 128, 128], [255, 128, 0], [255, 0, 128], [128, 255, 0],\n",
        "        [192, 192, 192], [128, 128, 128], [255, 165, 0], [255, 20, 147], [0, 191, 255]\n",
        "    ]\n",
        "\n",
        "    for i, mask in enumerate(masks):\n",
        "        color = colors[i % len(colors)]\n",
        "\n",
        "        # Get mask\n",
        "        m = mask['segmentation']\n",
        "\n",
        "        # Apply color to segmented image\n",
        "        segmented_image[m] = color\n",
        "\n",
        "        # Create overlay (blend with original)\n",
        "        alpha = 0.5\n",
        "        for c in range(3):\n",
        "            overlay_image[m, c] = (alpha * color[c] + (1 - alpha) * overlay_image[m, c])\n",
        "\n",
        "    # Convert to PIL Images\n",
        "    segmented_pil = Image.fromarray(segmented_image.astype(np.uint8))\n",
        "    overlay_pil = Image.fromarray(overlay_image.astype(np.uint8))\n",
        "\n",
        "    # Add text overlay showing model info\n",
        "    draw = ImageDraw.Draw(overlay_pil)\n",
        "    try:\n",
        "        font = ImageFont.load_default()\n",
        "    except:\n",
        "        font = None\n",
        "\n",
        "    text = f\"SAM Segments: {len(masks)}\"\n",
        "    draw.text((10, 10), text, fill=(255, 255, 255), font=font)\n",
        "    draw.text((11, 11), text, fill=(0, 0, 0), font=font)  # Shadow\n",
        "\n",
        "    return segmented_pil, overlay_pil\n",
        "\n",
        "def compare_sam_models(image, min_area=500):\n",
        "    \"\"\"Compare results from multiple SAM models\"\"\"\n",
        "    if image is None:\n",
        "        return \"âŒ No image provided\", image, image, image\n",
        "\n",
        "    # Get available models\n",
        "    available_models = [name for name, loaded in models_loaded.items() if loaded]\n",
        "\n",
        "    if len(available_models) < 1:\n",
        "        return \"âŒ Need at least 1 SAM model loaded for segmentation\", image, image, image\n",
        "\n",
        "    comparison_text = f\"# ğŸ”¥ **SAM Model Comparison Results**\\n\\n\"\n",
        "    comparison_text += f\"**Image analyzed with {len(available_models)} different SAM models:**\\n\\n\"\n",
        "\n",
        "    segmented_images = []\n",
        "\n",
        "    for model_name in available_models[:3]:  # Compare up to 3 models\n",
        "        try:\n",
        "            status, results, segmented_img, overlay_img, total_segments, _ = segment_objects_with_sam(\n",
        "                image, model_name, min_area\n",
        "            )\n",
        "\n",
        "            comparison_text += f\"## ğŸ¤– {model_name.upper()}\\n\"\n",
        "            comparison_text += f\"- **Segments Found:** {total_segments}\\n\"\n",
        "            comparison_text += f\"- **Model Type:** {model_name.replace('sam_vit_', 'ViT-').upper()}\\n\"\n",
        "            comparison_text += f\"- **Status:** {status.split(':')[-1].strip() if ':' in status else status}\\n\\n\"\n",
        "\n",
        "            segmented_images.append(overlay_img)\n",
        "\n",
        "        except Exception as e:\n",
        "            comparison_text += f\"## âŒ {model_name.upper()}\\n\"\n",
        "            comparison_text += f\"- **Error:** {str(e)}\\n\\n\"\n",
        "            segmented_images.append(image)\n",
        "\n",
        "    # Pad with original image if needed\n",
        "    while len(segmented_images) < 3:\n",
        "        segmented_images.append(image)\n",
        "\n",
        "    comparison_text += \"\\nğŸ’¡ **Tips for SAM Model Selection:**\\n\"\n",
        "    comparison_text += \"- **SAM ViT-B**: Fastest, good for real-time applications\\n\"\n",
        "    comparison_text += \"- **SAM ViT-L**: Balanced speed and accuracy\\n\"\n",
        "    comparison_text += \"- **SAM ViT-H**: Most accurate, slower processing\\n\"\n",
        "    comparison_text += \"- **Area Threshold**: Lower values detect smaller segments\\n\"\n",
        "\n",
        "    return comparison_text, segmented_images[0], segmented_images[1], segmented_images[2]\n",
        "\n",
        "def text_to_speech_multi_engine(text, language=\"en\", tts_engine=\"gtts\", voice_option=\"default\"):\n",
        "    \"\"\"Convert text to speech using multiple TTS engines\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"âŒ No text provided\", None\n",
        "\n",
        "    try:\n",
        "        audio_file = None\n",
        "\n",
        "        if tts_engine == \"gtts\" and tts_engines[\"gtts\"]:\n",
        "            # Google Text-to-Speech (online)\n",
        "            try:\n",
        "                # Map language codes for gTTS\n",
        "                gtts_lang_map = {\n",
        "                    \"en\": \"en\", \"es\": \"es\", \"fr\": \"fr\", \"de\": \"de\", \"it\": \"it\",\n",
        "                    \"pt\": \"pt\", \"ru\": \"ru\", \"zh\": \"zh\", \"ja\": \"ja\", \"ko\": \"ko\",\n",
        "                    \"hi\": \"hi\", \"ar\": \"ar\", \"nl\": \"nl\", \"sv\": \"sv\", \"no\": \"no\"\n",
        "                }\n",
        "\n",
        "                gtts_lang = gtts_lang_map.get(language, \"en\")\n",
        "\n",
        "                # Different voice options for gTTS (using different TLDs for variety)\n",
        "                tld_options = {\n",
        "                    \"default\": \"com\",\n",
        "                    \"uk\": \"co.uk\",\n",
        "                    \"australia\": \"com.au\",\n",
        "                    \"india\": \"co.in\",\n",
        "                    \"canada\": \"ca\"\n",
        "                }\n",
        "\n",
        "                tld = tld_options.get(voice_option, \"com\")\n",
        "\n",
        "                tts = gTTS(text=text, lang=gtts_lang, slow=False, tld=tld)\n",
        "                audio_file = os.path.join(temp_dir, f\"gtts_{hash(text)}.mp3\")\n",
        "                tts.save(audio_file)\n",
        "\n",
        "                status = f\"âœ… Google TTS ({voice_option}) generated successfully! ({len(text)} characters)\"\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"âŒ Google TTS Error: {str(e)}\", None\n",
        "\n",
        "        elif tts_engine == \"pyttsx3\" and tts_engines[\"pyttsx3\"]:\n",
        "            # Pyttsx3 (offline TTS)\n",
        "            try:\n",
        "                import pyttsx3\n",
        "\n",
        "                engine = pyttsx3.init()\n",
        "\n",
        "                # Get available voices\n",
        "                voices = engine.getProperty('voices')\n",
        "\n",
        "                # Voice selection logic\n",
        "                selected_voice = None\n",
        "                if voice_option == \"male\" and voices:\n",
        "                    # Try to find male voice\n",
        "                    for voice in voices:\n",
        "                        if 'male' in voice.name.lower() or 'david' in voice.name.lower():\n",
        "                            selected_voice = voice.id\n",
        "                            break\n",
        "                elif voice_option == \"female\" and voices:\n",
        "                    # Try to find female voice\n",
        "                    for voice in voices:\n",
        "                        if 'female' in voice.name.lower() or 'zira' in voice.name.lower() or 'susan' in voice.name.lower():\n",
        "                            selected_voice = voice.id\n",
        "                            break\n",
        "\n",
        "                if selected_voice:\n",
        "                    engine.setProperty('voice', selected_voice)\n",
        "\n",
        "                # Set speech rate and volume\n",
        "                rate_map = {\"slow\": 150, \"default\": 200, \"fast\": 250}\n",
        "                rate = rate_map.get(voice_option, 200)\n",
        "                engine.setProperty('rate', rate)\n",
        "                engine.setProperty('volume', 0.9)\n",
        "\n",
        "                audio_file = os.path.join(temp_dir, f\"pyttsx3_{hash(text)}.wav\")\n",
        "                engine.save_to_file(text, audio_file)\n",
        "                engine.runAndWait()\n",
        "\n",
        "                status = f\"âœ… Pyttsx3 TTS ({voice_option}) generated successfully! ({len(text)} characters)\"\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"âŒ Pyttsx3 TTS Error: {str(e)}\", None\n",
        "\n",
        "        elif tts_engine == \"edge-tts\" and tts_engines[\"edge-tts\"]:\n",
        "            # Microsoft Edge TTS (high quality)\n",
        "            try:\n",
        "                import edge_tts\n",
        "\n",
        "                # Voice mapping for Edge TTS\n",
        "                edge_voices = {\n",
        "                    \"en\": {\n",
        "                        \"female\": \"en-US-AriaNeural\",\n",
        "                        \"male\": \"en-US-GuyNeural\",\n",
        "                        \"child\": \"en-US-AnaNeural\",\n",
        "                        \"default\": \"en-US-JennyNeural\"\n",
        "                    },\n",
        "                    \"es\": {\n",
        "                        \"female\": \"es-ES-ElviraNeural\",\n",
        "                        \"male\": \"es-ES-AlvaroNeural\",\n",
        "                        \"default\": \"es-ES-ElviraNeural\"\n",
        "                    },\n",
        "                    \"fr\": {\n",
        "                        \"female\": \"fr-FR-DeniseNeural\",\n",
        "                        \"male\": \"fr-FR-HenriNeural\",\n",
        "                        \"default\": \"fr-FR-DeniseNeural\"\n",
        "                    },\n",
        "                    \"de\": {\n",
        "                        \"female\": \"de-DE-KatjaNeural\",\n",
        "                        \"male\": \"de-DE-ConradNeural\",\n",
        "                        \"default\": \"de-DE-KatjaNeural\"\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                voice_id = edge_voices.get(language, edge_voices[\"en\"]).get(voice_option, edge_voices.get(language, edge_voices[\"en\"])[\"default\"])\n",
        "\n",
        "                audio_file = os.path.join(temp_dir, f\"edge_{hash(text)}.mp3\")\n",
        "\n",
        "                # Run async function\n",
        "                async def generate_edge_tts():\n",
        "                    communicate = edge_tts.Communicate(text, voice_id)\n",
        "                    await communicate.save(audio_file)\n",
        "\n",
        "                # Run in event loop\n",
        "                try:\n",
        "                    loop = asyncio.get_event_loop()\n",
        "                    loop.run_until_complete(generate_edge_tts())\n",
        "                except RuntimeError:\n",
        "                    asyncio.run(generate_edge_tts())\n",
        "\n",
        "                status = f\"âœ… Edge TTS ({voice_option}) generated successfully! ({len(text)} characters)\"\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"âŒ Edge TTS Error: {str(e)}\", None\n",
        "        else:\n",
        "            return f\"âŒ TTS Engine '{tts_engine}' not available\", None\n",
        "\n",
        "        return status, audio_file\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ TTS Error: {str(e)}\", None\n",
        "\n",
        "def get_tts_voice_options(tts_engine):\n",
        "    \"\"\"Get available voice options for selected TTS engine\"\"\"\n",
        "    if tts_engine == \"gtts\":\n",
        "        return [\n",
        "            (\"Default\", \"default\"),\n",
        "            (\"UK English\", \"uk\"),\n",
        "            (\"Australian\", \"australia\"),\n",
        "            (\"Indian\", \"india\"),\n",
        "            (\"Canadian\", \"canada\")\n",
        "        ]\n",
        "    elif tts_engine == \"pyttsx3\":\n",
        "        return [\n",
        "            (\"Default Speed\", \"default\"),\n",
        "            (\"Male Voice\", \"male\"),\n",
        "            (\"Female Voice\", \"female\"),\n",
        "            (\"Slow Speed\", \"slow\"),\n",
        "            (\"Fast Speed\", \"fast\")\n",
        "        ]\n",
        "    elif tts_engine == \"edge-tts\":\n",
        "        return [\n",
        "            (\"Default\", \"default\"),\n",
        "            (\"Female Voice\", \"female\"),\n",
        "            (\"Male Voice\", \"male\"),\n",
        "            (\"Child Voice\", \"child\")\n",
        "        ]\n",
        "    else:\n",
        "        return [(\"Default\", \"default\")]\n",
        "\n",
        "def process_image_complete(image, ocr_lang, min_area, tts_lang, selected_model, tts_engine, voice_option):\n",
        "    \"\"\"Process image with OCR, SAM segmentation, and TTS\"\"\"\n",
        "    if image is None:\n",
        "        return \"âŒ Please upload an image first\", \"\", \"\", \"\", None, None, None, 0, 0, 0, 0\n",
        "\n",
        "    # Perform OCR\n",
        "    ocr_status, extracted_text, word_count, ocr_confidence = perform_ocr(image, ocr_lang)\n",
        "\n",
        "    # Perform SAM Segmentation with visualization\n",
        "    segment_status, segmentation_results, segmented_image, overlay_image, total_segments, _ = segment_objects_with_sam(\n",
        "        image, selected_model, min_area\n",
        "    )\n",
        "\n",
        "    # Create integrated results combining text and segments\n",
        "    integrated_results = create_integrated_results_sam(extracted_text, segmentation_results, total_segments, selected_model, tts_engine)\n",
        "\n",
        "    # Generate TTS if text was found\n",
        "    tts_status = \"\"\n",
        "    audio_file = None\n",
        "    if extracted_text.strip():\n",
        "        tts_status, audio_file = text_to_speech_multi_engine(extracted_text, tts_lang, tts_engine, voice_option)\n",
        "    else:\n",
        "        tts_status = \"âš ï¸ No text found for speech synthesis\"\n",
        "\n",
        "    # Combined status with better formatting\n",
        "    overall_status = f\"\"\"## ğŸ‰ **Processing Complete!**\n",
        "\n",
        "### ğŸ“ **OCR Results:**\n",
        "{ocr_status}\n",
        "\n",
        "### ğŸ§© **SAM Segmentation ({selected_model.upper()}):**\n",
        "{segment_status}\n",
        "\n",
        "### ğŸ”Š **Text-to-Speech ({tts_engine.upper()}):**\n",
        "{tts_status}\n",
        "\n",
        "---\n",
        "**ğŸ’¡ Tip:** Check the segmented images to see individual object masks!\"\"\"\n",
        "\n",
        "    return (overall_status, extracted_text, segmentation_results, integrated_results, audio_file,\n",
        "            segmented_image, overlay_image, word_count, ocr_confidence, total_segments, total_segments)\n",
        "\n",
        "def create_integrated_results_sam(extracted_text, segmentation_results, total_segments, model_name, tts_engine):\n",
        "    \"\"\"Create an integrated view of text and SAM segmentation results\"\"\"\n",
        "    integrated = f\"# ğŸ” **Complete Analysis Results**\\n\\n\"\n",
        "\n",
        "    # Summary section with model info\n",
        "    integrated += \"## ğŸ“Š **Quick Summary**\\n\"\n",
        "    integrated += f\"- ğŸ“ **Text**: {len(extracted_text.split()) if extracted_text.strip() else 0} words extracted\\n\"\n",
        "    integrated += f\"- ğŸ§© **Segments**: {total_segments} segments detected\\n\"\n",
        "    integrated += f\"- ğŸ¤– **SAM Model**: {model_name.upper()}\\n\"\n",
        "    integrated += f\"- ğŸ”Š **TTS Engine**: {tts_engine.upper()}\\n\"\n",
        "    integrated += f\"- ğŸµ **Audio**: {'Available' if extracted_text.strip() else 'No text to convert'}\\n\\n\"\n",
        "\n",
        "    # Text section\n",
        "    if extracted_text.strip():\n",
        "        integrated += \"## ğŸ“ **Extracted Text**\\n\"\n",
        "        integrated += f\"```\\n{extracted_text}\\n```\\n\\n\"\n",
        "    else:\n",
        "        integrated += \"âš ï¸ No text or segments detected in this image\\n\"\n",
        "        integrated += \"ğŸ’¡ **Suggestions:**\\n\"\n",
        "        integrated += \"- Try using a clearer, higher-resolution image\\n\"\n",
        "        integrated += \"- Lower the area threshold for more segments\\n\"\n",
        "        integrated += \"- Try a different SAM model (some are more sensitive)\\n\"\n",
        "        integrated += \"- Ensure the image contains recognizable objects or text\\n\"\n",
        "\n",
        "    # Segments section with model info\n",
        "    integrated += f\"## ğŸ§© **Image Segmentation ({model_name.upper()})**\\n\"\n",
        "    if total_segments > 0:\n",
        "        integrated += segmentation_results + \"\\n\\n\"\n",
        "    else:\n",
        "        integrated += \"*No segments detected with current area threshold*\\n\\n\"\n",
        "\n",
        "    # Audio section with engine info\n",
        "    integrated += f\"## ğŸ”Š **Audio Generation ({tts_engine.upper()})**\\n\"\n",
        "    if extracted_text.strip():\n",
        "        integrated += f\"âœ… Speech generated using **{tts_engine.upper()}** engine\\n\"\n",
        "        if tts_engine == \"gtts\":\n",
        "            integrated += \"ğŸŒ Google TTS: Natural-sounding online synthesis\\n\"\n",
        "        elif tts_engine == \"pyttsx3\":\n",
        "            integrated += \"ğŸ’» Pyttsx3: Fast offline synthesis\\n\"\n",
        "        elif tts_engine == \"edge-tts\":\n",
        "            integrated += \"ğŸ¤ Edge TTS: High-quality Microsoft neural voices\\n\"\n",
        "        integrated += \"ğŸµ Click the audio player to listen to the extracted text\\n\\n\"\n",
        "    else:\n",
        "        integrated += \"âš ï¸ No text available for speech synthesis\\n\\n\"\n",
        "\n",
        "    # Visual indication\n",
        "    integrated += \"## ğŸ‘ï¸ **Visual Results**\\n\"\n",
        "    integrated += \"âœ… Check the **'Segmented Image'** and **'Overlay'** tabs to see different visualizations\\n\"\n",
        "    integrated += \"ğŸ¨ **Segmented**: Each segment shown in different colors\\n\"\n",
        "    integrated += \"ğŸ­ **Overlay**: Segments blended with original image\\n\"\n",
        "    integrated += f\"ğŸ¤– SAM model: **{model_name.upper()}** with {total_segments} segments\\n\\n\"\n",
        "\n",
        "    # Combined insights\n",
        "    integrated += \"## ğŸ’¡ **Insights & Tips**\\n\"\n",
        "    if extracted_text.strip() and total_segments > 0:\n",
        "        integrated += \"âœ… This image contains both **text content** and **segmentable objects**\\n\"\n",
        "        integrated += f\"ğŸ”Š Listen to the extracted text using **{tts_engine.upper()}** synthesis\\n\"\n",
        "        integrated += \"ğŸ§© Try different SAM models to compare segmentation quality\\n\"\n",
        "        integrated += \"ğŸ¤ Try different TTS engines for varied audio quality and voices\\n\"\n",
        "        integrated += \"ğŸ“Š Adjust area threshold to control segment sensitivity\\n\"\n",
        "    elif extracted_text.strip():\n",
        "        integrated += \"ğŸ“ This image primarily contains **text content**\\n\"\n",
        "        integrated += f\"ğŸ”Š Listen to the text using **{tts_engine.upper()}** engine\\n\"\n",
        "        integrated += \"ğŸ¤ Try different TTS engines for different voice options\\n\"\n",
        "        integrated += \"ğŸ§© Consider using images with more distinct objects for segmentation demos\\n\"\n",
        "    elif total_segments > 0:\n",
        "        integrated += \"ğŸ§© This image contains **segmentable objects** but no readable text\\n\"\n",
        "        integrated += \"ğŸ“· Try using images with text for OCR and TTS functionality\\n\"\n",
        "        integrated += f\"ğŸ¤– **{model_name.upper()}** successfully segmented {total_segments} regions\\n\"\n",
        "        integrated += \"ğŸ”„ Try different SAM models to see how segmentation results vary\\n\"\n",
        "    else:\n",
        "        integrated += \"success\"\n",
        "    return integrated\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"ğŸ¤– AI Services Hub - SAM Edition\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ğŸ¤– AI Services Hub - SAM Edition\n",
        "        ### OCR Text Extraction â€¢ SAM Segmentation â€¢ Text-to-Speech\n",
        "\n",
        "        Upload an image to extract text, segment objects with pixel-perfect precision, and generate speech from the extracted text!\n",
        "        \"\"\")\n",
        "\n",
        "        # Model loading and selection\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                model_status = gr.Textbox(\n",
        "                    label=\"ğŸ¤– SAM Model Status\",\n",
        "                    value=\"Click buttons below to load Segment Anything models...\",\n",
        "                    interactive=False,\n",
        "                    lines=3\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸš€ **Load SAM Models**\")\n",
        "                with gr.Row():\n",
        "                    load_sam_b_btn = gr.Button(\"ğŸ“¦ SAM ViT-B (Fast)\", variant=\"secondary\", size=\"sm\")\n",
        "                    load_sam_l_btn = gr.Button(\"âš¡ SAM ViT-L (Better)\", variant=\"secondary\", size=\"sm\")\n",
        "                with gr.Row():\n",
        "                    load_sam_h_btn = gr.Button(\"ğŸ¯ SAM ViT-H (Best)\", variant=\"secondary\", size=\"sm\")\n",
        "                    load_all_sam_btn = gr.Button(\"ğŸ”¥ Load All SAM\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "        # TTS Engine Status and Loading\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                tts_status = gr.Textbox(\n",
        "                    label=\"ğŸ”Š TTS Engine Status\",\n",
        "                    value=\"Click button to initialize TTS engines...\",\n",
        "                    interactive=False,\n",
        "                    lines=3\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸ¤ **Initialize TTS Engines**\")\n",
        "                init_tts_btn = gr.Button(\"ğŸ”Š Initialize TTS Engines\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Left column - Input\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸ“¤ Input\")\n",
        "\n",
        "                image_input = gr.Image(\n",
        "                    label=\"ğŸ“· Upload Image\",\n",
        "                    type=\"pil\",\n",
        "                    height=300\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### âš™ï¸ Settings\")\n",
        "\n",
        "                # SAM model selection dropdown\n",
        "                model_selector = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        (\"SAM ViT-B (Fastest)\", \"sam_vit_b\"),\n",
        "                        (\"SAM ViT-L (Better Quality)\", \"sam_vit_l\"),\n",
        "                        (\"SAM ViT-H (Best Quality)\", \"sam_vit_h\")\n",
        "                    ],\n",
        "                    value=\"sam_vit_b\",\n",
        "                    label=\"ğŸ§© Select SAM Model\",\n",
        "                    info=\"Different models offer different speed/quality tradeoffs\"\n",
        "                )\n",
        "\n",
        "                # TTS Engine Selection\n",
        "                tts_engine_selector = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        (\"Google TTS (Online)\", \"gtts\"),\n",
        "                        (\"Pyttsx3 (Offline)\", \"pyttsx3\"),\n",
        "                        (\"Edge TTS (High Quality)\", \"edge-tts\")\n",
        "                    ],\n",
        "                    value=\"gtts\",\n",
        "                    label=\"ğŸ¤ Select TTS Engine\",\n",
        "                    info=\"Different engines offer different voice qualities and options\"\n",
        "                )\n",
        "\n",
        "                # Voice Options (dynamic based on TTS engine)\n",
        "                voice_selector = gr.Dropdown(\n",
        "                    choices=[(\"Default\", \"default\")],\n",
        "                    value=\"default\",\n",
        "                    label=\"ğŸµ Voice Options\",\n",
        "                    info=\"Available options vary by TTS engine\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    ocr_lang = gr.Dropdown(\n",
        "                        choices=[\n",
        "                            (\"English\", \"eng\"),\n",
        "                            (\"Spanish\", \"spa\"),\n",
        "                            (\"French\", \"fra\"),\n",
        "                            (\"German\", \"deu\"),\n",
        "                            (\"Italian\", \"ita\"),\n",
        "                            (\"Portuguese\", \"por\"),\n",
        "                            (\"Russian\", \"rus\"),\n",
        "                            (\"Chinese (Simplified)\", \"chi_sim\"),\n",
        "                            (\"Japanese\", \"jpn\"),\n",
        "                            (\"Korean\", \"kor\")\n",
        "                        ],\n",
        "                        value=\"eng\",\n",
        "                        label=\"ğŸ”¤ OCR Language\"\n",
        "                    )\n",
        "\n",
        "                    tts_lang = gr.Dropdown(\n",
        "                        choices=[\n",
        "                            (\"English\", \"en\"),\n",
        "                            (\"Spanish\", \"es\"),\n",
        "                            (\"French\", \"fr\"),\n",
        "                            (\"German\", \"de\"),\n",
        "                            (\"Italian\", \"it\"),\n",
        "                            (\"Portuguese\", \"pt\"),\n",
        "                            (\"Russian\", \"ru\"),\n",
        "                            (\"Chinese\", \"zh\"),\n",
        "                            (\"Japanese\", \"ja\"),\n",
        "                            (\"Korean\", \"ko\")\n",
        "                        ],\n",
        "                        value=\"en\",\n",
        "                        label=\"ğŸ”Š TTS Language\"\n",
        "                    )\n",
        "\n",
        "                min_area_threshold = gr.Slider(\n",
        "                    minimum=100,\n",
        "                    maximum=5000,\n",
        "                    step=100,\n",
        "                    value=500,\n",
        "                    label=\"ğŸ§© Minimum Segment Area (pixels)\",\n",
        "                    info=\"Lower = more small segments, Higher = only large segments\"\n",
        "                )\n",
        "\n",
        "                process_btn = gr.Button(\n",
        "                    \"ğŸš€ Process Image (All Services)\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            # Right column - Output\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸ“¤ Results\")\n",
        "\n",
        "                status_output = gr.Markdown(label=\"ğŸ“Š Status\")\n",
        "\n",
        "                # Statistics\n",
        "                with gr.Row():\n",
        "                    word_count_out = gr.Number(label=\"ğŸ“ Words Found\", interactive=False)\n",
        "                    ocr_confidence_out = gr.Number(label=\"ğŸ¯ OCR Confidence %\", interactive=False)\n",
        "                    segments_count_out = gr.Number(label=\"ğŸ§© Segments Found\", interactive=False)\n",
        "                    coverage_out = gr.Number(label=\"ğŸ“Š Coverage %\", interactive=False)\n",
        "\n",
        "        # SAM Model Information Panel\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### ğŸ§© **SAM Model Comparison**\n",
        "\n",
        "                | Model | Speed | Quality | Memory | Use Case |\n",
        "                |-------|-------|---------|--------|----------|\n",
        "                | **SAM ViT-B** | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | ğŸŒŸğŸŒŸğŸŒŸ | Low | Real-time, mobile |\n",
        "                | **SAM ViT-L** | ğŸŒŸğŸŒŸğŸŒŸ | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | Medium | Balanced applications |\n",
        "                | **SAM ViT-H** | ğŸŒŸğŸŒŸ | ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ | High | Best quality, research |\n",
        "\n",
        "                **SAM vs YOLO**: SAM provides pixel-perfect segmentation masks while YOLO gives bounding boxes with object classification.\n",
        "                \"\"\")\n",
        "\n",
        "        # Image Results Section (Full Width) - Updated for SAM\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### ğŸ–¼ï¸ **Visual Results - Original vs Segmented**\")\n",
        "                with gr.Tab(\"ğŸ“· Original Image\"):\n",
        "                    original_display = gr.Image(label=\"Original Image\", interactive=False)\n",
        "                with gr.Tab(\"ğŸ§© Segmented Image\"):\n",
        "                    segmented_output = gr.Image(\n",
        "                        label=\"Segmented Image (Color-coded)\",\n",
        "                        interactive=False,\n",
        "                        height=400\n",
        "                    )\n",
        "                with gr.Tab(\"ğŸ­ Overlay Image\"):\n",
        "                    overlay_output = gr.Image(\n",
        "                        label=\"Overlay (Segments + Original)\",\n",
        "                        interactive=False,\n",
        "                        height=400\n",
        "                    )\n",
        "\n",
        "        # Model Comparison Section - Updated for SAM\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### ğŸ”¥ **SAM Model Comparison** (Compare Different SAM Models)\")\n",
        "                compare_btn = gr.Button(\"âš”ï¸ Compare All Loaded SAM Models\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Tab(\"SAM ViT-B Result\"):\n",
        "                        comparison_img1 = gr.Image(label=\"ViT-B Segmentation\", interactive=False)\n",
        "                    with gr.Tab(\"SAM ViT-L Result\"):\n",
        "                        comparison_img2 = gr.Image(label=\"ViT-L Segmentation\", interactive=False)\n",
        "                    with gr.Tab(\"SAM ViT-H Result\"):\n",
        "                        comparison_img3 = gr.Image(label=\"ViT-H Segmentation\", interactive=False)\n",
        "\n",
        "                comparison_results = gr.Markdown(label=\"ğŸ“Š SAM Comparison Results\")\n",
        "\n",
        "        # Integrated Results Section (Full Width)\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### ğŸ” **Integrated Results - Text & Segments Together**\")\n",
        "                integrated_output = gr.Markdown(\n",
        "                    label=\"Complete Analysis\",\n",
        "                    value=\"Upload an image and click 'Process Image' to see integrated results here...\"\n",
        "                )\n",
        "\n",
        "        # Audio and Individual Results (Side by Side)\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸ”Š **Generated Audio**\")\n",
        "                audio_out = gr.Audio(\n",
        "                    label=\"ğŸµ Listen to Extracted Text\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                # TTS Testing Section\n",
        "                gr.Markdown(\"### ğŸ¤ **Test TTS Engines**\")\n",
        "                test_text = gr.Textbox(\n",
        "                    label=\"Test Text\",\n",
        "                    value=\"Hello, this is a test of the text-to-speech system.\",\n",
        "                    lines=2\n",
        "                )\n",
        "                test_tts_btn = gr.Button(\"ğŸµ Test Selected TTS Engine\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ğŸ“ **Raw Text Output**\")\n",
        "                extracted_text_out = gr.Textbox(\n",
        "                    label=\"Extracted Text (Raw)\",\n",
        "                    lines=4,\n",
        "                    max_lines=8,\n",
        "                    interactive=False,\n",
        "                    placeholder=\"Extracted text will appear here...\"\n",
        "                )\n",
        "\n",
        "        # Segmentation Results (Full Width)\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### ğŸ§© **Segmentation Details**\")\n",
        "                segmentation_out = gr.Textbox(\n",
        "                    label=\"Segmentation Results (Detailed View)\",\n",
        "                    lines=6,\n",
        "                    max_lines=10,\n",
        "                    interactive=False,\n",
        "                    placeholder=\"Segmentation results will appear here...\"\n",
        "                )\n",
        "\n",
        "        # Individual service buttons\n",
        "        with gr.Row():\n",
        "            ocr_btn = gr.Button(\"ğŸ“ OCR Only\", variant=\"secondary\")\n",
        "            segment_btn = gr.Button(\"ğŸ§© Segment Only\", variant=\"secondary\")\n",
        "            tts_btn = gr.Button(\"ğŸ”Š TTS Only\", variant=\"secondary\")\n",
        "\n",
        "        # Event handlers for TTS engine initialization\n",
        "        init_tts_btn.click(\n",
        "            fn=initialize_tts_engines,\n",
        "            outputs=[tts_status]\n",
        "        )\n",
        "\n",
        "        # Update voice options when TTS engine changes\n",
        "        def update_voice_options(engine):\n",
        "            return gr.Dropdown.update(choices=get_tts_voice_options(engine), value=\"default\")\n",
        "\n",
        "        tts_engine_selector.change(\n",
        "            fn=update_voice_options,\n",
        "            inputs=[tts_engine_selector],\n",
        "            outputs=[voice_selector]\n",
        "        )\n",
        "\n",
        "        # Event handlers for SAM model loading\n",
        "        load_sam_b_btn.click(\n",
        "            fn=lambda: initialize_sam_models(\"sam_vit_b\"),\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        load_sam_l_btn.click(\n",
        "            fn=lambda: initialize_sam_models(\"sam_vit_l\"),\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        load_sam_h_btn.click(\n",
        "            fn=lambda: initialize_sam_models(\"sam_vit_h\"),\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        load_all_sam_btn.click(\n",
        "            fn=load_all_sam_models,\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        # Main processing event handler\n",
        "        process_btn.click(\n",
        "            fn=process_image_complete,\n",
        "            inputs=[image_input, ocr_lang, min_area_threshold, tts_lang, model_selector, tts_engine_selector, voice_selector],\n",
        "            outputs=[status_output, extracted_text_out, segmentation_out, integrated_output, audio_out,\n",
        "                    segmented_output, overlay_output, word_count_out, ocr_confidence_out, segments_count_out, coverage_out]\n",
        "        )\n",
        "\n",
        "        # Update original image display when new image is uploaded\n",
        "        image_input.change(\n",
        "            fn=lambda img: img,\n",
        "            inputs=[image_input],\n",
        "            outputs=[original_display]\n",
        "        )\n",
        "\n",
        "        # Model comparison handler\n",
        "        compare_btn.click(\n",
        "            fn=compare_sam_models,\n",
        "            inputs=[image_input, min_area_threshold],\n",
        "            outputs=[comparison_results, comparison_img1, comparison_img2, comparison_img3]\n",
        "        )\n",
        "\n",
        "        # Individual service handlers\n",
        "        ocr_btn.click(\n",
        "            fn=lambda img, lang: perform_ocr(img, lang),\n",
        "            inputs=[image_input, ocr_lang],\n",
        "            outputs=[status_output, extracted_text_out, word_count_out, ocr_confidence_out]\n",
        "        )\n",
        "\n",
        "        segment_btn.click(\n",
        "            fn=lambda img, model, area: segment_objects_with_sam(img, model, area),\n",
        "            inputs=[image_input, model_selector, min_area_threshold],\n",
        "            outputs=[status_output, segmentation_out, segmented_output, overlay_output, segments_count_out, coverage_out]\n",
        "        )\n",
        "\n",
        "        tts_btn.click(\n",
        "            fn=lambda text, lang, engine, voice: text_to_speech_multi_engine(text, lang, engine, voice),\n",
        "            inputs=[extracted_text_out, tts_lang, tts_engine_selector, voice_selector],\n",
        "            outputs=[status_output, audio_out]\n",
        "        )\n",
        "\n",
        "        # Test TTS handler\n",
        "        test_tts_btn.click(\n",
        "            fn=lambda text, lang, engine, voice: text_to_speech_multi_engine(text, lang, engine, voice),\n",
        "            inputs=[test_text, tts_lang, tts_engine_selector, voice_selector],\n",
        "            outputs=[status_output, audio_out]\n",
        "        )\n",
        "\n",
        "        # Add examples\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"Sample image with text and objects\", \"eng\", 500, \"en\", \"sam_vit_b\", \"gtts\", \"default\"]\n",
        "            ],\n",
        "            inputs=[image_input, ocr_lang, min_area_threshold, tts_lang, model_selector, tts_engine_selector, voice_selector],\n",
        "            label=\"ğŸ’¡ Try these examples (upload your own images)\"\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### ğŸ“– How to Use:\n",
        "        1. **Initialize TTS engines** using the TTS initialization button\n",
        "        2. **Load SAM models** using the buttons above (start with SAM ViT-B for speed)\n",
        "        3. **Upload an image** using the image input\n",
        "        4. **Select your preferred models** from the dropdowns (SAM for segmentation, TTS for speech)\n",
        "        5. **Choose voice options** based on your selected TTS engine\n",
        "        6. **Adjust area threshold** for segment sensitivity\n",
        "        7. **Click \"Process Image\"** to run all AI services at once\n",
        "        8. **View visual results** in the Original, Segmented, and Overlay tabs\n",
        "        9. **Compare models** using the SAM comparison feature\n",
        "        10. **Test TTS engines** using the test section with custom text\n",
        "\n",
        "        ### ğŸ§© SAM Model Information:\n",
        "        - **SAM ViT-B (Base)**: Fastest processing, good for real-time applications, ~375MB\n",
        "        - **SAM ViT-L (Large)**: Balanced speed and quality, good all-around choice, ~1.25GB\n",
        "        - **SAM ViT-H (Huge)**: Highest quality, slower processing, best for detailed analysis, ~2.4GB\n",
        "\n",
        "        ### ğŸ¯ SAM vs YOLO Comparison:\n",
        "\n",
        "        | Feature | YOLO | SAM |\n",
        "        |---------|------|-----|\n",
        "        | **Output** | Bounding boxes + labels | Pixel-perfect masks |\n",
        "        | **Speed** | Very fast | Moderate |\n",
        "        | **Precision** | Object-level | Pixel-level |\n",
        "        | **Use Case** | Object detection/counting | Image segmentation/editing |\n",
        "        | **Classes** | Predefined (80 classes) | Class-agnostic (any object) |\n",
        "\n",
        "        ### ğŸ¤ TTS Engine Details:\n",
        "\n",
        "        #### **Google TTS (gTTS)**\n",
        "        - ğŸŒ **Online synthesis** with natural voices\n",
        "        - ğŸ—ºï¸ **Regional accents**: US, UK, Australian, Indian, Canadian\n",
        "        - ğŸ¯ **Best for**: Natural-sounding speech, multiple language support\n",
        "        - âš¡ **Speed**: Moderate (requires internet)\n",
        "\n",
        "        #### **Pyttsx3 (Offline TTS)**\n",
        "        - ğŸ’» **Offline synthesis** - no internet required\n",
        "        - ğŸšï¸ **Voice control**: Male/Female voices, speed adjustment\n",
        "        - ğŸ¯ **Best for**: Privacy, offline use, fast processing\n",
        "        - âš¡ **Speed**: Very fast (local processing)\n",
        "\n",
        "        #### **Edge TTS (Microsoft)**\n",
        "        - ğŸ§  **Neural voices** with high quality synthesis\n",
        "        - ğŸ‘¥ **Multiple personas**: Female, Male, Child voices per language\n",
        "        - ğŸ¯ **Best for**: Highest quality speech, professional applications\n",
        "        - âš¡ **Speed**: Fast (requires internet)\n",
        "\n",
        "        ### ğŸ› ï¸ Individual Services:\n",
        "        - **OCR Only**: Extract text from the image\n",
        "        - **Segment Only**: Generate pixel-perfect object masks using SAM\n",
        "        - **TTS Only**: Convert the extracted text to speech using selected engine\n",
        "        - **Test TTS**: Try different engines with custom text\n",
        "        - **Compare SAM Models**: See how different SAM versions perform on the same image\n",
        "\n",
        "        ### âœ¨ Key Features:\n",
        "        - ğŸ§© **Pixel-Perfect Segmentation**: SAM provides precise object boundaries, not just boxes\n",
        "        - ğŸ¤– **Multiple SAM Models**: Compare SAM ViT-B, ViT-L, and ViT-H variants\n",
        "        - ğŸ¨ **Dual Visualizations**: Color-coded segments and overlay views\n",
        "        - ğŸ¤ **Multiple TTS Engines**: Choose from Google TTS, Pyttsx3, or Edge TTS\n",
        "        - ğŸµ **Voice Variety**: Different voice options for each TTS engine\n",
        "        - ğŸ” **Model Comparison**: Side-by-side results from different SAM models\n",
        "        - ğŸ“Š **Detailed Segment Info**: Area, stability scores, and bounding boxes\n",
        "        - ğŸ­ **Visual Overlays**: See segments blended with original image\n",
        "        - ğŸ”Š **Instant Audio**: Generated speech plays automatically\n",
        "        - ğŸ“Š **Real-time Stats**: Live word count, confidence, and segment counts\n",
        "        - ğŸŒ **Multi-language**: Support for 10+ languages in both OCR and TTS\n",
        "        - ğŸ“± **Mobile-friendly**: Works perfectly on phones and tablets\n",
        "\n",
        "        ### ğŸ’¡ Tips:\n",
        "        - **Start with SAM ViT-B** for fast results, then try ViT-L or ViT-H for better quality\n",
        "        - **Lower area threshold** (100-300) to catch smaller segments, higher (1000+) for only large objects\n",
        "        - **Compare SAM models** on the same image to see how segmentation quality varies\n",
        "        - **Try different TTS engines** to find your preferred voice quality and style\n",
        "        - **Google TTS** for natural voices, **Pyttsx3** for offline use, **Edge TTS** for highest quality\n",
        "        - **Use high-resolution images** for better text OCR and more precise segmentation\n",
        "        - **Check both visualizations**: Segmented shows pure masks, Overlay shows context\n",
        "        - **SAM is class-agnostic**: It finds object boundaries without knowing what objects are\n",
        "        - **Test TTS engines** with your own text to compare voice quality\n",
        "        - **SAM works best** on images with clear object boundaries and good contrast\n",
        "        \"\"\")\n",
        "\n",
        "    return interface\n",
        "\n",
        "def find_free_gradio_port(start_port=7860, max_attempts=50):\n",
        "    \"\"\"Find a free port for Gradio starting from start_port\"\"\"\n",
        "    import socket\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('', port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            continue\n",
        "    raise RuntimeError(f\"Could not find a free port after {max_attempts} attempts starting from {start_port}\")\n",
        "\n",
        "# Main execution\n",
        "print(\"ğŸš€ Starting Complete AI Services with SAM Segmentation...\")\n",
        "\n",
        "# Create and launch interface\n",
        "interface = create_interface()\n",
        "\n",
        "# Launch with automatic port detection\n",
        "print(\"ğŸŒ Launching Gradio interface with SAM integration...\")\n",
        "\n",
        "try:\n",
        "    available_port = find_free_gradio_port()\n",
        "    print(f\"ğŸ” Found available port: {available_port}\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"âŒ Port error: {e}\")\n",
        "    available_port = None\n",
        "\n",
        "if available_port:\n",
        "    try:\n",
        "        interface.launch(\n",
        "            share=True,  # Creates public link automatically\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=available_port,\n",
        "            show_error=True,\n",
        "            quiet=False,\n",
        "            prevent_thread_lock=True  # Allows notebook to remain interactive\n",
        "        )\n",
        "\n",
        "        print(\"ğŸ‰ Gradio interface is now live!\")\n",
        "        print(f\"ğŸŒ Running on port: {available_port}\")\n",
        "        print(\"ğŸ“± You can access it from the public URL shown above\")\n",
        "        print(\"ğŸ§© SAM-powered segmentation interface ready!\")\n",
        "        print(\"ğŸ”— The interface provides OCR, SAM segmentation, and TTS services\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to launch Gradio: {e}\")\n",
        "        print(\"ğŸ”§ Try restarting your Colab runtime and re-running the cell\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Could not find available port for Gradio\")\n",
        "    print(\"ğŸ”§ Please restart your Colab runtime and try again\")\n",
        "\n",
        "print(\"\\nğŸš€ **Complete SAM-Powered AI Services Ready!**\")\n",
        "print(\"ğŸ§© Features: OCR + Segment Anything + Multi-TTS\")\n",
        "print(\"ğŸ’¡ Click 'Initialize TTS Engines' and 'Load SAM Models' in the interface first!\")\n",
        "print(\"ğŸ“– Instructions:\")\n",
        "print(\"1. Initialize TTS engines\")\n",
        "print(\"2. Load SAM ViT-B model (fastest)\")\n",
        "print(\"3. Upload an image\")\n",
        "print(\"4. Adjust area threshold (500 is good start)\")\n",
        "print(\"5. Click 'Process Image' to run all services\")\n",
        "print(\"6. View results in Original/Segmented/Overlay tabs\")\n",
        "print(\"7. Listen to generated audio from extracted text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hi54x78Ue-Yo",
        "outputId": "87895038-95ec-420f-f333-10a221f94a9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pyttsx3\n",
            "  Downloading pyttsx3-2.99-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting edge-tts\n",
            "  Downloading edge_tts-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting asyncio\n",
            "  Downloading asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Collecting click<8.2,>=7.1 (from gtts)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (3.12.15)\n",
            "Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (2025.8.3)\n",
            "Requirement already satisfied: tabulate<1.0.0,>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading pyttsx3-2.99-py3-none-any.whl (32 kB)\n",
            "Downloading edge_tts-7.2.0-py3-none-any.whl (30 kB)\n",
            "Downloading asyncio-4.0.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyttsx3, pytesseract, click, asyncio, gtts, edge-tts\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "Successfully installed asyncio-4.0.0 click-8.1.8 edge-tts-7.2.0 gtts-2.5.4 pytesseract-0.3.13 pyttsx3-2.99\n",
            "Collecting segment-anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segment-anything, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 segment-anything-1.0\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-gghbxu3_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-gghbxu3_\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,923 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [36.0 kB]\n",
            "Fetched 2,424 kB in 1s (1,667 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "tesseract-ocr-eng set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak-dev libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 6 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,575 kB of archives.\n",
            "After this operation, 3,802 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak-data amd64 1.48.15+dfsg-3 [1,085 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libespeak1 amd64 1.48.15+dfsg-3 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 espeak amd64 1.48.15+dfsg-3 [64.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libespeak-dev amd64 1.48.15+dfsg-3 [193 kB]\n",
            "Fetched 1,575 kB in 1s (2,140 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../1-libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../2-espeak-data_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../3-libespeak1_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../4-espeak_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking espeak (1.48.15+dfsg-3) ...\n",
            "Selecting previously unselected package libespeak-dev:amd64.\n",
            "Preparing to unpack .../5-libespeak-dev_1.48.15+dfsg-3_amd64.deb ...\n",
            "Unpacking libespeak-dev:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-data:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up libespeak1:amd64 (1.48.15+dfsg-3) ...\n",
            "Setting up espeak (1.48.15+dfsg-3) ...\n",
            "Setting up libespeak-dev:amd64 (1.48.15+dfsg-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "ğŸ“¥ Downloading SAM model checkpoints...\n",
            "âœ… SAM models downloaded successfully!\n",
            "ğŸš€ Starting Complete AI Services with SAM Segmentation...\n",
            "ğŸŒ Launching Gradio interface with SAM integration...\n",
            "ğŸ” Found available port: 7860\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b99849f010cdb036ea.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b99849f010cdb036ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ Gradio interface is now live!\n",
            "ğŸŒ Running on port: 7860\n",
            "ğŸ“± You can access it from the public URL shown above\n",
            "ğŸ§© SAM-powered segmentation interface ready!\n",
            "ğŸ”— The interface provides OCR, SAM segmentation, and TTS services\n",
            "\n",
            "ğŸš€ **Complete SAM-Powered AI Services Ready!**\n",
            "ğŸ§© Features: OCR + Segment Anything + Multi-TTS\n",
            "ğŸ’¡ Click 'Initialize TTS Engines' and 'Load SAM Models' in the interface first!\n",
            "ğŸ“– Instructions:\n",
            "1. Initialize TTS engines\n",
            "2. Load SAM ViT-B model (fastest)\n",
            "3. Upload an image\n",
            "4. Adjust area threshold (500 is good start)\n",
            "5. Click 'Process Image' to run all services\n",
            "6. View results in Original/Segmented/Overlay tabs\n",
            "7. Listen to generated audio from extracted text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHLsuK5ahxM_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}